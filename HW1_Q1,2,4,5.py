# -*- coding: utf-8 -*-
"""HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iLx_bJ4wahWSGke5P73Z-SQh7_BJhq7v
"""

"""Question 1.2"""

import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

"""Q1 Consider the case when y(x) = x + sin(1.5x) + N (0, 0.3), where N (0, 0.3) is normal distri-
bution with mean 0 and variance 0.3. Here f (x) = x + sin(1.5x) and Îµ = N (0, 0.3). Create a dataset of
size 20 points by randomly generating samples from y. Display the dataset and f (x). Use scatter plot
for y and smooth line plot for f (x).  """

np.random.seed(42)  # For reproducibility
x_values = np.random.uniform(-10, 10, 20)

# Compute f(x) = x + sin(1.5x)
def f(x):
    return x + np.sin(1.5 * x)

# Generate noise from a normal distribution N(0, 0.3)
noise = np.random.normal(0, np.sqrt(0.3), 20)


y_values = f(x_values) + noise

plt.scatter(x_values, y_values, color='blue', label='y(x) with noise', zorder=2)

x_smooth = np.linspace(-10, 10, 400)
f_smooth = f(x_smooth)
plt.plot(x_smooth, f_smooth, color='red', label='f(x) without noise', zorder=1)

plt.title('Scatter Plot of y(x) and Line Plot of f(x)')

plt.legend()
plt.grid(True)
plt.show()



"""Question 1.3"""
# Fit degree 1 polynomial
g1_coefficients = np.polyfit(x_smooth, f_smooth, 1)
g1 = np.poly1d(g1_coefficients)

# Fit g2 degree 2 polynomial
g3_coefficients = np.polyfit(x_smooth, f_smooth, 3)
g3 = np.poly1d(g3_coefficients)

# Fit g2 degree 2 polynomial
g10_coefficients = np.polyfit(x_smooth, f_smooth, 10)
g10 = np.poly1d(g10_coefficients)

print("Degree 1 coefficients: " , g1_coefficients)

print("Degree 3 coefficients: " , g3_coefficients)

print("Degree 10 coefficients: " , g10_coefficients)
# Step 4: Plot the results

plt.figure(figsize=(5, 3))

# Plot g1(x) (degree 1 polynomial)
plt.title('Polynomial Estimators for g1')
plt.scatter(x_values, y_values, color='blue', label='y(x) with noise', zorder=2)
plt.plot(x_smooth, g1(x_smooth), color='green', linestyle='--', label='g1(x) - Degree 1', linewidth=2)

plt.show()
plt.figure(figsize=(5, 3))
# Plot g3(x) (degree 3 polynomial)
plt.title('Polynomial Estimators for g2')
plt.scatter(x_values, y_values, color='blue', label='y(x) with noise', zorder=2)
plt.plot(x_smooth, g3(x_smooth), color='orange', linestyle='--', label='g3(x) - Degree 3', linewidth=2)

plt.show()
plt.figure(figsize=(5, 3))
# Plot g10(x) (degree 10 polynomial)
plt.title('Polynomial Estimators for g3')
plt.scatter(x_values, y_values, color='blue', label='y(x) with noise', zorder=2)
plt.plot(x_smooth, g10(x_smooth), color='purple', linestyle='--', label='g10(x) - Degree 10', linewidth=2)

plt.show()
# # Customize the plot
# plt.title('Polynomial Estimators for f(x)')
# plt.xlabel('x')
# plt.ylabel('y')
# plt.legend()
# plt.grid(True)

# # Show the plot
# plt.show()


# Conclusions:
# g1 underfits the data and isnt able to capture the complexity of the true function.
# g3 fits well
# g10 over fits the data and tries to model each nuance in variation of f(x)


#Create 100 datasets of size 50:
datasets = []
for _ in range(100):
    x_values = np.random.uniform(-10, 10, 50)
    noise = np.random.normal(0, np.sqrt(0.3), 50)
    y_values = f(x_values) + noise
    datasets.append((x_values, y_values))

max_degree = 15
bias_squared = np.zeros(max_degree)
variance = np.zeros(max_degree)
error = np.zeros(max_degree)

"""Question 1.4"""
from sklearn.metrics import mean_squared_error, r2_score
from mlxtend.evaluate import bias_variance_decomp
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

x = 20 * np.random.random(50)
x = np.sort(x)
f = lambda x: x + np.sin(1.5*x)
dataset = []
degree = range(1,16)

for i in range(100):
    y = f(x) + np.random.normal(0, 0.3, 50)
    dataset.append(y)

x_reshaped = x.reshape(-1,1)
dataset2 = np.array(dataset)

mse_list = []
bias_list = []
variance_list = []

# Split into test and train in 20 - 80 ratio
for j in range(100):

    X_train, X_test, y_train, y_test = train_test_split(x_reshaped, dataset2[j], test_size=0.20, random_state=0)


    for i in degree:
        polynomial_features= PolynomialFeatures(degree=i)
        x_poly = polynomial_features.fit_transform(x_reshaped)
        model = LinearRegression()
        model.fit(X_train, y_train)


        mse, bias, var = bias_variance_decomp(model, X_train, y_train.flatten(), X_test, y_test.flatten(), loss='mse', num_rounds=50)
        mse_list.append(mse)
        bias_list.append(bias**2)
        variance_list.append(var)

# Initialize lists to store averaged metrics
mse_averaged = []
bias_averaged = []
variance_averaged = []

# Set the range for averaging
start_index = 0
end_index = 100

# Averaging the metrics for different model complexities
while end_index <= len(mse_list):
    mse_averaged.append(np.mean(mse_list[start_index:end_index]))
    bias_averaged.append(np.mean(bias_list[start_index:end_index]))  # Use bias_squared_list for bias
    variance_averaged.append(np.mean(variance_list[start_index:end_index]))

    # Move to the next segment of 100
    start_index += 100
    end_index += 100

complexity=range(1,16)

#Plotting
fig = plt.figure(figsize=(8,4))
plt.plot(complexity,mse_averaged,label='MSE')
plt.plot(complexity,bias_averaged,label='Squared Bias')
plt.plot(complexity,variance_averaged, label='Variance')
plt.grid()
plt.legend()

"""Question 1.5"""
from sklearn.model_selection import train_test_split, cross_val_score
from statistics import mean
from sklearn.linear_model import Ridge

x = np.random.random(50)
fx = x + np.sin(1.5*x)
e = np.random.normal(0, 0.3, 50)
y = fx + e
x_reshaped = x.reshape(-1,1)

X_train, X_test, y_train, y_test = train_test_split(x_reshaped, y, test_size=0.20)
poly = PolynomialFeatures(degree=10)
x_poly = poly.fit_transform(x_reshaped)
linear_regression = LinearRegression()
linear_regression.fit(x_reshaped, y)

rp = linear_regression.predict(X_test)

mse, bias, var = bias_variance_decomp(linear_regression, X_train, y_train, X_test, y_test, loss = 'mse', num_rounds=200, random_seed=1)

print('MSE: %.4f' % mse)
print('Bias: %.4f' % bias)
print('Variance: %.4f' % var)
print('\n')

cross_val_scores_ridge = []
alpha = []

for i in range(1, 9):
    ridgeModel = Ridge(alpha = i * 0.25)
    ridgeModel.fit(X_train, y_train)
    scores = cross_val_score(ridgeModel, x_reshaped, y, cv = 10)
    avg_cross_val_score = mean(scores)*100
    cross_val_scores_ridge.append(avg_cross_val_score)
    alpha.append(i * 0.25)

ridgeModelChosen = Ridge(alpha = 2)
ridgeModelChosen.fit(X_train, y_train)

rp = ridgeModelChosen.predict(X_test)
mse2, bias2, var2 = bias_variance_decomp(ridgeModel, X_train, y_train, X_test, y_test, loss='mse', num_rounds=200, random_seed=1)
# summarize results
print('MSE Regularized: %.3f' % mse2)
print('Bias Regularized: %.3f' % bias2)
print('Variance Regularized: %.3f' % var2)

"""Question 2"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, precision_recall_curve
from sklearn.metrics import roc_auc_score, auc, confusion_matrix
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import fetch_openml


dataset = fetch_openml(data_id=1464)
X = dataset.data
y = dataset.target.astype(int) - 1 # Adjust target labels from {1, 2} to {0, 1}

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train two classifiers
adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)
log_reg = LogisticRegression(max_iter=1000, random_state=42)

adaboost.fit(X_train, y_train)
log_reg.fit(X_train, y_train)

# Get prediction probabilities
y_scores_adaboost = adaboost.predict_proba(X_test)[:, 1]
y_scores_log_reg = log_reg.predict_proba(X_test)[:, 1]

# Calculate ROC and PR curves for both classifiers
fpr_adaboost, tpr_adaboost, _ = roc_curve(y_test, y_scores_adaboost)
fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, y_scores_log_reg)

precision_adaboost, recall_adaboost, _ = precision_recall_curve(y_test, y_scores_adaboost)
precision_log_reg, recall_log_reg, _ = precision_recall_curve(y_test, y_scores_log_reg)

# Plot ROC curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(fpr_adaboost, tpr_adaboost, label="Adaboost", color='blue')
plt.plot(fpr_log_reg, tpr_log_reg, label="Logistic Regression", color='green')
plt.title("ROC Curves")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()

# Plot PR curves
plt.subplot(1, 2, 2)
plt.plot(recall_adaboost, precision_adaboost, label="Adaboost", color='blue')
plt.plot(recall_log_reg, precision_log_reg, label="Logistic Regression", color='green')
plt.title("PR Curves")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()

plt.tight_layout()
plt.show()

# Find and mark the point for an all-positive classifier
positive_rate = np.sum(y_test) / len(y_test)
print(f"Positive Rate (All Positive Classifier Precision): {positive_rate}")

# Add all-positive classifier points to the plots
plt.figure(figsize=(12, 5))

# For an all-positive classifier:
# TPR = 1 (because all positives are correctly identified).
# FPR = 1 (because all negatives are incorrectly classified as positives).
# Hence, the all-positive classifier is represented by the point (1, 1) on the ROC curve.

# Recall is always 1.
# Precision is equal to the positive rate of the dataset (since it predicts all samples as positive).
# Therefore, the point for an all-positive classifier on the PR curve is (1, positive rate

# ROC Curve with all-positive classifier point
plt.subplot(1, 2, 1)
plt.plot(fpr_adaboost, tpr_adaboost, label="Adaboost", color='blue')
plt.plot(fpr_log_reg, tpr_log_reg, label="Logistic Regression", color='green')
plt.scatter(1, 1, color='red', label="All-Positive Classifier (1, 1)", zorder=5)
plt.title("ROC Curves (All-Positive Classifier Marked)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()

# PR Curve with all-positive classifier point
plt.subplot(1, 2, 2)
plt.plot(recall_adaboost, precision_adaboost, label="Adaboost", color='blue')
plt.plot(recall_log_reg, precision_log_reg, label="Logistic Regression", color='green')
plt.scatter(1, positive_rate, color='red', label=f"All-Positive Classifier (Precision = {positive_rate:.2f})", zorder=5)
plt.title("PR Curves (All-Positive Classifier Marked)")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.legend()

plt.tight_layout()
plt.show()

# Calculate AUROC
auroc_adaboost = roc_auc_score(y_test, y_scores_adaboost)
auroc_log_reg = roc_auc_score(y_test, y_scores_log_reg)

# Calculate AUPR
aupr_adaboost = auc(recall_adaboost, precision_adaboost)
aupr_log_reg = auc(recall_log_reg, precision_log_reg)

## AdaBoost (PRG)

precision_gain_adaboost = (precision_adaboost - np.pi)/((1-np.pi)*precision_adaboost)
recall_gain_adaboost = (recall_adaboost - np.pi)/((1-np.pi)*recall_adaboost)

#Logistic Regression (PRG)

precision_gain_log_reg = (precision_log_reg - np.pi)/((1-np.pi)*precision_log_reg)
recall_gain_log_reg = (recall_log_reg - np.pi)/((1-np.pi)*recall_log_reg)

#Plotting PRG
plt.title('PR Gain Curve')
plt.xlabel('Recall Gain')
plt.ylabel('Precision Gain')
plt.plot(recall_gain_adaboost, precision_gain_adaboost, label='AdaBoost')
plt.plot(recall_gain_log_reg, precision_gain_log_reg, label='LogisticRegression')
plt.grid()

#AUPRG
auprg_adaboost = auc(recall_gain_adaboost, precision_gain_adaboost)
auprg_log_reg = auc(recall_gain_log_reg, precision_gain_log_reg)


# Display Results
print(f"AUROC for Adaboost: {auroc_adaboost:.4f}")
print(f"AUROC for Logistic Regression: {auroc_log_reg:.4f}")
print(f"AUPR for Adaboost: {aupr_adaboost:.4f}")
print(f"AUPR for Logistic Regression: {aupr_log_reg:.4f}")
print(f"AUPRG for Adaboost: {auprg_adaboost:.4f}")
print(f"AUPRG for Logistic Regression: {auprg_log_reg:.4f}")











import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras

# Load the Fashion MNIST dataset
fashion_mnist = keras.datasets.fashion_mnist
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Class names for the Fashion MNIST dataset
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Print the shapes of the training and test sets
print(x_train.shape)  # Shape of training images
print(y_test.shape)   # Shape of test labels
print(np.unique(y_train, return_counts=True))  # Unique classes in training labels
print(np.unique(y_test, return_counts=True))   # Unique classes in test labels

# Display 3 representative images
fig = plt.figure(figsize=(8, 10))
plt.subplot(3, 1, 1)
plt.imshow(x_train[0])
plt.colorbar()
plt.subplot(3, 1, 2)
plt.imshow(x_train[1])
plt.colorbar()
plt.subplot(3, 1, 3)
plt.imshow(x_train[3])
plt.colorbar()
plt.grid(False)
plt.show()

pip install --upgrade tensorflow

from tensorflow.keras import layers, models
from tensorflow.keras import backend as K

class MiniGoogLeNet:
    @staticmethod
    def build(width, height, depth, classes):
        model = models.Sequential()
        model.add(layers.Conv2D(32, (3, 3), padding='same', input_shape=(height, width, depth)))
        model.add(layers.Activation('relu'))
        model.add(layers.MaxPooling2D(pool_size=(2, 2)))

        model.add(layers.Conv2D(64, (3, 3), padding='same'))
        model.add(layers.Activation('relu'))
        model.add(layers.MaxPooling2D(pool_size=(2, 2)))

        model.add(layers.Flatten())
        model.add(layers.Dense(classes))
        model.add(layers.Activation('softmax'))

        return model

from tensorflow.keras.callbacks import Callback

class CyclicLR(Callback):
    def __init__(self, mode='triangular', base_lr=0.001, max_lr=0.006, step_size=2000,
                 scale_fn=None, scale_mode='cycle'):
        super(CyclicLR, self).__init__()
        self.mode = mode
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size = step_size
        self.iteration = 0
        self.history = {}

    def on_train_begin(self, logs=None):
        logs = logs or {}
        self.iteration = 0
        self.history['learning_rate'] = []

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        self.iteration += 1
        lr = self.get_learning_rate()
        self.history['learning_rate'].append(lr)
        K.set_value(self.model.optimizer.lr, lr)

    def get_lr(self):
        cycle = np.floor(1 + self.iteration / (2 * self.step_size))
        x = np.abs(self.iteration / self.step_size - 2 * cycle + 1)
        if self.mode == 'triangular':
            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))
        else:
            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))

from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import numpy as np
import argparse
import cv2
import sys
import sys
sys.argv=['']
del sys

ap = argparse.ArgumentParser()
ap.add_argument("-f", "--lr-find", type=int, default=0,
	help="whether or not to find optimal learning rate")
args = vars(ap.parse_args())

# load the training and testing data
print("[INFO] loading Fashion MNIST data...")
((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()
# Fashion MNIST images are 28x28 but the network we will be training
# is expecting 32x32 images
trainX = np.array([cv2.resize(x, (32, 32)) for x in trainX])
testX = np.array([cv2.resize(x, (32, 32)) for x in testX])
# scale the pixel intensities to the range [0, 1]
trainX = trainX.astype("float") / 255.0
testX = testX.astype("float") / 255.0
# reshape the data matrices to include a channel dimension (required
# for training)
trainX = trainX.reshape((trainX.shape[0], 32, 32, 1))
testX = testX.reshape((testX.shape[0], 32, 32, 1))
# convert the labels from integers to vectors
lb = LabelBinarizer()
trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)
# construct the image generator for data augmentation
aug = ImageDataGenerator(width_shift_range=0.1,
	height_shift_range=0.1, horizontal_flip=True,
	fill_mode="nearest")
# initialize the optimizer and model

opt = SGD(learning_rate=MIN_LR, momentum=0.9)
model = MiniGoogLeNet.build(width=32, height=32, depth=1, classes=10)
model.compile(loss="categorical_crossentropy", optimizer=opt,
	metrics=["accuracy"])


stepSize = STEP_SIZE * (trainX.shape[0] // BATCH_SIZE)
clr = CyclicLR(
	mode='triangular',
	base_lr=1e-10,
	max_lr=10,
	step_size=10)
# train the network
print("[INFO] training network...")
H = model.fit(
	x=aug.flow(trainX, trainY, batch_size=BATCH_SIZE),
	validation_data=(testX, testY),
	steps_per_epoch=trainX.shape[0] // BATCH_SIZE,
	epochs=5,
	callbacks=[clr],
	verbose=1)
# evaluate the network and show a classification report
print("[INFO] evaluating network...")
predictions = model.predict(x=testX, batch_size=BATCH_SIZE)
print(classification_report(testY.argmax(axis=1),
	predictions.argmax(axis=1), target_names=CLASSES))

pip install torch torchvision matplotlib

import torch
import torchvision.transforms as transforms
from torchvision.datasets import FashionMNIST
from torch.utils.data import DataLoader

# Define transformations
transform = transforms.Compose([transforms.ToTensor()])

# Load datasets
train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

